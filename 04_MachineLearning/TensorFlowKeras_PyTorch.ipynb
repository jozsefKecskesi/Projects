{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with TensorFlow/Keras and PyTorch\n",
    "### IMPOTANT Note: Both TensorFlow/Keras and PyTorch paths included in parallel. Chose one path at once and follow that, then go through on the other path separately if you would.\n",
    "\n",
    "**Tensors**\n",
    "\n",
    "*   **Concept:** Tensors are multi-dimensional arrays, similar to NumPy's `ndarray`.  They are the fundamental data structure in deep learning.\n",
    "*   **TensorFlow/Keras:**\n",
    "\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "\n",
    "    # Create tensors\n",
    "    scalar = tf.constant(5)  # 0-dimensional tensor (scalar)\n",
    "    vector = tf.constant([1, 2, 3])  # 1-dimensional tensor (vector)\n",
    "    matrix = tf.constant([[1, 2], [3, 4]])  # 2-dimensional tensor (matrix)\n",
    "    tensor3d = tf.constant([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # 3-dimensional tensor\n",
    "\n",
    "    print(\"Scalar:\", scalar)\n",
    "    print(\"Vector:\", vector)\n",
    "    print(\"Matrix:\", matrix)\n",
    "    print(\"3D Tensor:\", tensor3d)\n",
    "\n",
    "    # Basic operations\n",
    "    print(\"Addition:\", vector + 5)\n",
    "    print(\"Multiplication:\", matrix * 2)\n",
    "    print(\"Matrix Multiplication:\", tf.matmul(matrix, matrix))\n",
    "\n",
    "    # Convert between NumPy arrays and TensorFlow tensors\n",
    "    numpy_array = np.array([1, 2, 3])\n",
    "    tf_tensor = tf.convert_to_tensor(numpy_array)\n",
    "    numpy_back = tf_tensor.numpy()\n",
    "\n",
    "    # Check the shape and data type\n",
    "    print(\"Shape:\", matrix.shape)\n",
    "    print(\"Data type:\", matrix.dtype)\n",
    "\n",
    "    # Using GPUs (if available)\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        with tf.device('/GPU:0'):\n",
    "            matrix_gpu = tf.constant([[1, 2], [3, 4]])\n",
    "            print(\"Matrix on GPU:\", matrix_gpu)\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "    import numpy as np\n",
    "\n",
    "    # Create tensors\n",
    "    scalar = torch.tensor(5)\n",
    "    vector = torch.tensor([1, 2, 3])\n",
    "    matrix = torch.tensor([[1, 2], [3, 4]])\n",
    "    tensor3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\n",
    "\n",
    "    print(\"Scalar:\", scalar)\n",
    "    print(\"Vector:\", vector)\n",
    "    print(\"Matrix:\", matrix)\n",
    "    print(\"3D Tensor:\", tensor3d)\n",
    "\n",
    "    # Basic operations\n",
    "    print(\"Addition:\", vector + 5)\n",
    "    print(\"Multiplication:\", matrix * 2)\n",
    "    print(\"Matrix Multiplication:\", torch.matmul(matrix, matrix))  # or matrix @ matrix\n",
    "\n",
    "    # Convert between NumPy arrays and PyTorch tensors\n",
    "    numpy_array = np.array([1, 2, 3])\n",
    "    torch_tensor = torch.from_numpy(numpy_array)  # Shares memory (if on CPU)\n",
    "    numpy_back = torch_tensor.numpy()\n",
    "\n",
    "    # Check the shape and data type\n",
    "    print(\"Shape:\", matrix.shape)  # or matrix.size()\n",
    "    print(\"Data type:\", matrix.dtype)\n",
    "\n",
    "    # Using GPUs (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        matrix_gpu = matrix.to(device)  # Move tensor to GPU\n",
    "        # or matrix_gpu = torch.tensor([[1, 2], [3, 4]], device=device) # Create directly on GPU\n",
    "        print(\"Matrix on GPU:\", matrix_gpu)\n",
    "\n",
    "        # Operations on GPU\n",
    "        result_gpu = matrix_gpu * 2\n",
    "        print(\"Result on GPU\", result_gpu)\n",
    "\n",
    "        # Move back to CPU\n",
    "        result_cpu = result_gpu.cpu()\n",
    "        print(\"Result back to CPU\", result_cpu)\n",
    "\n",
    "    ```\n",
    "\n",
    "**Neural Network Basics**\n",
    "\n",
    "*   **Neurons:**  The basic building block of a neural network.  A neuron takes inputs, multiplies them by weights, adds a bias, and applies an activation function.\n",
    "\n",
    "*   **Activation Functions:** Introduce non-linearity into the network, allowing it to learn complex patterns.\n",
    "    *   **ReLU (Rectified Linear Unit):**  `f(x) = max(0, x)` (most common)\n",
    "    *   **Sigmoid:** `f(x) = 1 / (1 + exp(-x))` (outputs values between 0 and 1, often used in output layer for binary classification)\n",
    "    *   **Tanh (Hyperbolic Tangent):** `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))` (outputs values between -1 and 1)\n",
    "\n",
    "*   **Layers:**\n",
    "    *   **Dense (Fully Connected):** Each neuron in the layer is connected to every neuron in the previous layer.\n",
    "    *   **Convolutional (Conv2D):**  Used for image processing, applies filters to extract features.\n",
    "    *   **Recurrent (LSTM, GRU):**  Used for sequence data, has internal memory to process sequences of varying lengths.\n",
    "\n",
    "*   **Loss Functions:** Measure the difference between the model's predictions and the true values.\n",
    "    *   **Mean Squared Error (MSE):**  Common for regression.\n",
    "    *   **Binary Cross-Entropy:** Common for binary classification.\n",
    "    *   **Categorical Cross-Entropy:** Common for multi-class classification.\n",
    "\n",
    "*   **Optimizers:**  Algorithms that adjust the model's weights to minimize the loss function.\n",
    "    *   **SGD (Stochastic Gradient Descent):**  Basic optimizer.\n",
    "    *   **Adam (Adaptive Moment Estimation):**  Popular and often performs well (adaptive learning rates).\n",
    "    * **RMSprop:** Another optimizer with adaptive learning rates.\n",
    "\n",
    "**Building and Training Models**\n",
    "\n",
    "*   **TensorFlow/Keras (Sequential API):**  Simple way to build models layer by layer.\n",
    "\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Sequential([\n",
    "        Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to a 784-dimensional vector\n",
    "        Dense(128, activation='relu'),  # Dense layer with 128 neurons and ReLU activation\n",
    "        Dense(10, activation='softmax')  # Output layer with 10 neurons (for 10 classes) and softmax activation\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',  # For integer labels\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Load and preprocess the MNIST dataset\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(x_test[:5])\n",
    "    print(\"Predictions:\", np.argmax(predictions, axis=1)) # Get the class with the highest probability\n",
    "    print(\"True Labels:\", y_test[:5])\n",
    "    ```\n",
    "\n",
    "* **TensorFlow/Keras (Functional API):** More flexible, allows for complex model architectures (e.g., models with multiple inputs or outputs).\n",
    "    ```python\n",
    "    from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    # Define the input\n",
    "    input_tensor = Input(shape=(28,28))\n",
    "\n",
    "    # Define the layers\n",
    "    x = Flatten()(input_tensor)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output_tensor = Dense(10, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    # Compile and train (same as Sequential API)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torchvision import datasets, transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Define the model (as a class inheriting from nn.Module)\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.fc1 = nn.Linear(28 * 28, 128)  # Fully connected layer\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            x = F.relu(self.fc1(x))  # Apply ReLU activation\n",
    "            x = self.fc2(x)\n",
    "            return F.log_softmax(x, dim=1)  # Apply log_softmax for numerical stability\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = Net()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    # Define the loss function\n",
    "    criterion = nn.CrossEntropyLoss() # Combines LogSoftmax and NLLLoss\n",
    "\n",
    "    # Load and preprocess the MNIST dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # Normalize\n",
    "    ])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "      model.train() # Set the model to training mode\n",
    "      for batch_idx, (data, target) in enumerate(train_loader):\n",
    "          data, target = data.to(device), target.to(device) # Move data to device\n",
    "          optimizer.zero_grad()  # Zero the gradients\n",
    "          output = model(data)\n",
    "          loss = criterion(output, target)\n",
    "          loss.backward()  # Backpropagation\n",
    "          optimizer.step()  # Update weights\n",
    "          if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "\n",
    "    # Test Loop\n",
    "    def test(model, device, test_loader, criterion):\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():  # Disable gradient calculation during testing\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "    # Move model to device (GPU if available)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Run training and testing\n",
    "    epochs = 5\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "        test(model, device, test_loader, criterion)\n",
    "    ```\n",
    "\n",
    "**Data Loading and Preprocessing**\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "    *   `tf.data.Dataset`:  Efficient way to create data pipelines.  Handles loading, preprocessing, batching, and shuffling.\n",
    "\n",
    "        ```python\n",
    "        # Create a dataset from a NumPy array\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        dataset = dataset.shuffle(buffer_size=10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # Load data from files (e.g., images)\n",
    "        # list_ds = tf.data.Dataset.list_files('path/to/images/*.jpg')\n",
    "        # def process_path(file_path):\n",
    "        #  # Load and preprocess image\n",
    "        #  img = tf.io.read_file(file_path)\n",
    "        #  img = tf.image.decode_jpeg(img, channels=3)\n",
    "        #  img = tf.image.resize(img, [224, 224])\n",
    "        #   img = tf.cast(img, tf.float32) / 255.0 # Normalize\n",
    "        #   label = ...  # Extract label from file path or other source\n",
    "        #  return img, label\n",
    "        # image_ds = list_ds.map(process_path).batch(32)\n",
    "\n",
    "        # ... then use dataset in model.fit()\n",
    "        # model.fit(dataset, epochs=...)\n",
    "        ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "    *   `torch.utils.data.Dataset`:  Abstract class representing a dataset.  You create a custom dataset class by inheriting from `Dataset` and implementing `__len__` and `__getitem__`.\n",
    "    *   `torch.utils.data.DataLoader`:  Provides an iterable over a dataset, handling batching, shuffling, and parallel data loading.\n",
    "\n",
    "        ```python\n",
    "        # (See the PyTorch example in Module 3 for a complete example using\n",
    "        # torchvision.datasets.MNIST and DataLoader)\n",
    "\n",
    "        # Example of a custom dataset\n",
    "        from torch.utils.data import Dataset, DataLoader\n",
    "        class CustomDataset(Dataset):\n",
    "            def __init__(self, data, targets, transform=None):\n",
    "                self.data = data\n",
    "                self.targets = targets\n",
    "                self.transform = transform\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                sample = self.data[idx]\n",
    "                target = self.targets[idx]\n",
    "                if self.transform:\n",
    "                    sample = self.transform(sample)\n",
    "                return sample, target\n",
    "\n",
    "        # Usage\n",
    "        # custom_dataset = CustomDataset(data, targets, transform=...)\n",
    "        # data_loader = DataLoader(custom_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        # ... then use data_loader in the training loop\n",
    "        ```\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "*   **Concept:**  CNNs are designed for processing data with a grid-like topology, such as images.  They use convolutional layers to extract local features and pooling layers to reduce dimensionality.\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "\n",
    "    ```python\n",
    "    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # 32 filters, 3x3 kernel\n",
    "        MaxPooling2D((2, 2)),  # 2x2 pooling\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Reshape MNIST data to include a channel dimension (required for Conv2D)\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "    x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    class CNN(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, kernel_size=3)  # 1 input channel, 32 output channels, 3x3 kernel\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "            self.fc1 = nn.Linear(1600, 10) # 64 channels * 5 * 5 after two pooling layers\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), 2))  # Convolution -> ReLU -> Max Pooling\n",
    "            x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "            x = x.view(-1, 1600)  # Flatten\n",
    "            x = self.fc1(x)\n",
    "            return F.log_softmax(x, dim=1)\n",
    "\n",
    "    # (Training loop is similar to the previous PyTorch example)\n",
    "    model = CNN()\n",
    "    # ... optimizer, loss, data loading ...\n",
    "    ```\n",
    "\n",
    "**Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "*   **Concept:** RNNs are designed for processing sequential data. They have recurrent connections that allow information to persist across time steps.  LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) are variants of RNNs that address the vanishing gradient problem.\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "\n",
    "    ```python\n",
    "    from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Embedding, Dense\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        Embedding(input_dim=10000, output_dim=32),  # Embedding layer for text data\n",
    "        LSTM(64),  # LSTM layer with 64 units\n",
    "        # Or: SimpleRNN(64)\n",
    "        # Or: GRU(64)\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Example with dummy sequence data (replace with your actual data)\n",
    "    x_train = np.random.randint(0, 10000, size=(1000, 50))  # 1000 sequences of length 50\n",
    "    y_train = np.random.randint(0, 2, size=(1000, 1)) # Binary Labels\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "            super(RNN, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            # Or: self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            # Or: self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Initialize hidden and cell states\n",
    "            h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "            # Forward propagate LSTM\n",
    "            out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "\n",
    "            # Decode the hidden state of the last time step\n",
    "            out = self.fc(out[:, -1, :])\n",
    "            return out\n",
    "\n",
    "    # Example usage\n",
    "    model = RNN(input_size=28, hidden_size=128, num_layers=2, num_classes=10)\n",
    "\n",
    "    # (Training loop is similar to the previous PyTorch examples)\n",
    "    ```\n",
    "\n",
    "**Transfer Learning**\n",
    "\n",
    "*   **Concept:** Reuse a pre-trained model (trained on a large dataset) as a starting point for a new task.  This can significantly improve performance and reduce training time, especially when you have limited data.\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "\n",
    "    ```python\n",
    "    from tensorflow.keras.applications import VGG16  # Example: VGG16 pre-trained on ImageNet\n",
    "    from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    # Load the pre-trained VGG16 model (excluding the top classification layer)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the base model's weights (don't train them)\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Add custom layers on top\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  # Pool the feature maps\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(10, activation='softmax')(x)  # Output layer for 10 classes\n",
    "\n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # (Load and preprocess your data, then train the model)\n",
    "    # model.fit(...)\n",
    "\n",
    "    # Fine-tuning (optional): Unfreeze some of the top layers of the base model\n",
    "    # and train with a very low learning rate.\n",
    "    # for layer in base_model.layers[:15]: # Example: Freeze first 15 layers\n",
    "    #   layer.trainable = False\n",
    "    # for layer in base_model.layers[15:]:\n",
    "    #    layer.trainable = True\n",
    "\n",
    "    # from tensorflow.keras.optimizers import Adam\n",
    "    # model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy']) # Low Learning Rate\n",
    "\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    import torchvision.models as models\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # Load a pre-trained ResNet18 model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze the base model's weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the final fully connected layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 10)  # New FC layer for 10 classes\n",
    "\n",
    "    # (Move model to device, define optimizer and loss, load data, and train)\n",
    "    # ...\n",
    "    # Fine-tuning (Optional)\n",
    "    # for param in model.fc.parameters(): # Unfreeze the last layer\n",
    "    #   param.requires_grad = True\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0001) # Use a smaller learning rate.\n",
    "\n",
    "    ```\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "*   **Concept:** Techniques to prevent overfitting.\n",
    "\n",
    "*   **Dropout:**  Randomly \"drop out\" (set to zero) neurons during training.  This forces the network to learn more robust features.\n",
    "\n",
    "    ```python\n",
    "    # TensorFlow/Keras\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "    model = keras.Sequential([\n",
    "        # ... other layers ...\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout with a rate of 0.5 (50% of neurons dropped)\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # PyTorch\n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "          super().__init__()\n",
    "          # ... other layers ...\n",
    "          self.fc1 = nn.Linear(128, 64)\n",
    "          self.dropout = nn.Dropout(0.5)\n",
    "          self.fc2 = nn.Linear(64, 10)\n",
    "        def forward(self,x):\n",
    "          # ...\n",
    "          x = F.relu(self.fc1(x))\n",
    "          x = self.dropout(x)\n",
    "          x = self.fc2(x)\n",
    "          return x\n",
    "    ```\n",
    "\n",
    "*   **L1/L2 Regularization:** Add a penalty to the loss function based on the magnitude of the weights.  L1 regularization encourages sparsity (some weights become zero), while L2 regularization encourages smaller weights.\n",
    "\n",
    "    ```python\n",
    "    # TensorFlow/Keras\n",
    "    from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        # ... other layers ...\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization\n",
    "        # Or: Dense(128, activation='relu', kernel_regularizer=l1(0.01)),  # L1 regularization\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # PyTorch (add to the loss)\n",
    "    l2_lambda = 0.01\n",
    "    l1_lambda = 0.01\n",
    "    l2_reg = torch.tensor(0., requires_grad=True)\n",
    "    l1_reg = torch.tensor(0., requires_grad=True)\n",
    "    for param in model.parameters():\n",
    "        l2_reg = l2_reg + torch.norm(param, 2) #L2\n",
    "        l1_reg = l1_reg + torch.norm(param, 1) #L1\n",
    "\n",
    "    loss = criterion(output, target) + l2_lambda * l2_reg + l1_lambda * l1_reg # Add to loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ```\n",
    "\n",
    "**Model Saving and Loading**\n",
    "\n",
    "*   **TensorFlow/Keras:**\n",
    "\n",
    "    ```python\n",
    "    # Save the entire model (architecture, weights, optimizer state)\n",
    "    model.save('my_model.h5')  # Or: model.save('my_model') (SavedModel format)\n",
    "\n",
    "    # Load the model\n",
    "    loaded_model = keras.models.load_model('my_model.h5')\n",
    "\n",
    "    # Save only the weights\n",
    "    model.save_weights('my_model_weights.h5')\n",
    "    # Load only the weights (requires the model architecture to be defined first)\n",
    "    # model.load_weights('my_model_weights.h5')\n",
    "\n",
    "    ```\n",
    "\n",
    "*   **PyTorch:**\n",
    "\n",
    "    ```python\n",
    "    # Save the model's state dictionary (recommended)\n",
    "    torch.save(model.state_dict(), 'my_model.pth')\n",
    "\n",
    "    # Load the model's state dictionary\n",
    "    model = Net()  # Create an instance of the model architecture\n",
    "    model.load_state_dict(torch.load('my_model.pth'))\n",
    "    model.eval() # Important: Set to evaluation mode after loading\n",
    "\n",
    "    # Save the entire model (less flexible)\n",
    "    # torch.save(model, 'my_model_full.pth')\n",
    "    # loaded_model = torch.load('my_model_full.pth')\n",
    "    ```\n",
    "\n",
    "This course provides a comprehensive introduction to deep learning with TensorFlow/Keras and PyTorch. The parallel presentation of code for both frameworks allows for easy comparison and helps you decide which one best fits your needs and preferred coding style. Remember to practice by building and training your own models, experimenting with different architectures and hyperparameters, and exploring the extensive documentation and resources available for both frameworks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
